{
 "cells": [
 {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####        This GitHub Repository accompanies the Paper\n",
    "## **How to Design and Employ Specialized Large Language Models for Accounting and Tax Research: The Example of TaxBERT**\n",
    "**Frank Hechtner, Lukas Schmidt, Andreas Seebeck, and Marius Wei√ü**\n",
    "##### If the following Guide/Repository is used for academic or scientific purposes, please cite the paper Hechtner et al., (2025) How to Design and Employ Specialized Large Language Models for Accounting and Tax Research: The Example of TaxBERT.\n",
    "##### Link to paper: SSRN/Arxiv\n",
    "##### Version as of February 2025\n",
    "## Part 1 of example code: Domain adaptive pretraining\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook aims to give a comprehensive 10 step guide for accounting and tax researchers for domain-adaptive pretraining.\n",
    "\n",
    "1. **Library Imports**: Import necessary libraries from PyTorch and Hugging Face for handling large language models (LLMs) and datasets.\n",
    "\n",
    "2. **Hardware Setup**: Detect the presence of an NVIDIA GPU to determine the computation device - CUDA for GPU.\n",
    "\n",
    "3. **Data Loading**: Initialize directories and loads .txt files, handle them with lowercasing and encoding correction.\n",
    "\n",
    "4. **Text Preprocessing**: Check text and BERT compatibility and defining a process for extracting important tokens for vocabulary enhancement.\n",
    "\n",
    "5. **Redundancy Reduction** with DataSelector class.\n",
    "\n",
    "6. **Tokenization**: Convert texts into tokenized datasets and get important tokens.\n",
    "\n",
    "7. **Model Setup**: Initialize and customize a DistilRoBERTa model for domain-specific training.\n",
    "\n",
    "8. **Optimizer & Scheduler**: Configure the AdamW optimizer and implement custom learning rate schedulers.\n",
    "\n",
    "9. **Reproducible Training** Loop with Early Stopping: Define optimal hyperparameter.\n",
    "\n",
    "10. **Model Evaluation** and Saving.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####**1. Library Imports**\n",
    "The following libraries and imports are required to start. Additionally, **PyTorch, Huggingface transformers and NVIDIA CUDA** are mandatory depenencies.\n",
    "**Note**: The example code requires Python 3.9 or later. It is tested on the stable PyTorch 2.6.0, Transformers 4.48.3, and NVIDIA CUDA 12.4.\n",
    "We cannot guarantee the stability for newer or older versions of these packages.\n",
    "Note: There are several great websites to start learning Python, depending on your learning style and goals. Here are some recommendations:\n",
    "\n",
    "[Real Python](https://realpython.com/)\n",
    "\n",
    "[Codecademy](https://www.codecademy.com/)\n",
    "\n",
    "[DataCamp](https://www.datacamp.com/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2839e5e-a061-4519-8b6f-03ef4073f6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "import re\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
    "from torch.optim import AdamW\n",
    "from transformers import AdamW, Trainer, TrainingArguments, AutoTokenizer, DataCollatorWithPadding\n",
    "from transformers import RobertaForMaskedLM, DataCollatorForLanguageModeling\n",
    "from datasets import load_dataset, Dataset\n",
    "from collections import Counter\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilroberta-base')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####**2. Hardware Setup**\n",
    "Training a BERT model involves extensive matrix multiplications and tensor operations, which are highly parallelizable. Thus, training is significantly faster on a **NVIDIA GPU** compared to a CPU.\n",
    "For training, we used an RTX 4090 GPU, an Intel i9-14900K CPU, and 128 GB of RAM.\n",
    "The GPU, particularly its VRAM, plays a crucial role in determining the batch size, which directly impacts training speed - insufficient VRAM can significantly slow down or even hinder training.\n",
    "We recommend at least 12 GB of VRAM (such as an RTX A2000 or RTX 3080) for a corpus of similar size to TaxBERT (20 million tokens). For an initial assessment of suitable \n",
    "NVIDIA GPUs, refer to the [CUDA Compute Capability list](https://developer.nvidia.com/cuda-gpus). Higher Compute Capability generally indicates better performance.\n",
    "Once PyTorch and CUDA are installed, we determine whether a compatible NVIDIA GPU is available for computations and set the device accordingly:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8dafea1-91f1-4137-b295-91d16ce692ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####**3. Data Loading**\n",
    "In the following step, we define input folders, as well as the lists that will later contain the training and evaluation texts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321b0fe6-c858-4576-9852-c0379ed8279d",
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = [\n",
    "    r'C:\\Data_Source_Folder_I',\n",
    "    r'C:\\Data_Source_Folder_II',\n",
    "    r'C:\\Data_Source_Folder_III',\n",
    "]\n",
    "\n",
    "train_texts = []\n",
    "eval_texts = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, we assume that all relevant files are in the form of .txt files. However, you can also use other file formats.\n",
    "Note: Using .txt files is preferable because they are lightweight, universally compatible, and free from formatting artifacts that might interfere with text processing.\n",
    "Unlike PDFs or Word documents, which may contain hidden metadata, non-standard encoding, or structural elements, plain text files ensure clean and consistent input for NLP tasks.\n",
    "The function, **load_text_files(directory)**, takes a folder path as input. It scans the folder for files and selects only those with a .txt extension.\n",
    "For each of these files, it attempts to read the content using the UTF-8 encoding. If this fails due to encoding issues, it tries again with the Latin-1 encoding.\n",
    "If that also fails, it makes a final attempt with the Windows-1252 encoding.\n",
    "The text from the files is processed to ensure consistency: it is converted to lowercase.\n",
    "Note: Different data sources might require different preprocessing solutions; there is no one-size-fits-all procedure.\n",
    "Successfully processed texts are then added to a list. Finally, the function returns this list of processed text contents.\n"
   ]
  },
  {
       "cell_type": "code",
       "execution_count": null,
       "id": "17631a2f-0075-4630-a4a1-286298aed8e5",
       "metadata": {},
       "outputs": [],
       "source": [
        "import os\n",
        "\n",
        "def load_text_files(directory):\n",
        "     texts = []\n",
        "     for filename in os.listdir(directory):\n",
        "             if filename.endswith(\".txt\"):\n",
        "                     file_path = os.path.join(directory, filename)\n",
        "                     try:\n",
        "                             with open(file_path, 'r', encoding='utf-8') as file:\n",
        "                                     text = file.read().lower()\n",
        "                                     texts.append(text)\n",
        "                     except (UnicodeDecodeError, FileNotFoundError, PermissionError) as e1:\n",
        "                             try:\n",
        "                                     with open(file_path, 'r', encoding='latin-1') as file:\n",
        "                                             text = file.read().lower()\n",
        "                                             texts.append(text)\n",
        "                             except (UnicodeDecodeError, FileNotFoundError, PermissionError) as e2:\n",
        "                                     try:\n",
        "                                             with open(file_path, 'r', encoding='windows-1252') as file:\n",
        "                                                     text = file.read().lower()\n",
        "                                                     texts.append(text)\n",
        "                                     except (UnicodeDecodeError, FileNotFoundError, PermissionError) as e3:\n",
        "                                             print(f\"{filename} {e3}\")\n",
        "     return texts"
       ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####**4. Text Preprocessing**\n",
    "BERT models, including variants like DistilBERT and RoBERTa, have a maximum input limit of 512 tokens.\n",
    "If a text exceeds this limit, BERT will either truncate the excess tokens or throw an error during processing.\n",
    "The **chunk_text function** is designed to split long text into smaller chunks that fit within this 512-token limit.\n",
    "This ensures that longer documents can be processed without losing important information or causing errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e03e53-6d1a-4571-9181-1b83425e91db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to split text into chunks\n",
    "def chunk_text(text, chunk_size=512):\n",
    "    tokens = tokenizer.encode(text, add_special_tokens=False)\n",
    "    return [tokens[i:i+chunk_size] for i in range(0, len(tokens), chunk_size)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **clean_token** function is used to clean and standardize tokens to ensure that identical words are not treated as different tokens due to capitalization or trailing punctuation.\n",
    "It performs two main tasks: first, it converts the token to lowercase, ensuring that words like 'Tax' and 'tax' are treated as the same.\n",
    "Second, it removes specific punctuation marks: periods (.), commas (,), exclamation points (!), and question marks (?) - if they appear at the end of the token.\n",
    "For example, 'Profit.' becomes 'profit', 'Revenue,' becomes 'revenue', and 'taxes?' becomes 'taxes'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad001cf1-d402-4bac-8541-5771e2f852c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean tokens for new token extraction\n",
    "def clean_token(token):\n",
    "    return re.sub(r'[.,!?]$', '', token.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function **get_important_tokens** is designed to identify and extract the vocabulary to be added during the process of vocabulary augmentation.\n",
    "Focusing solely on adding terms simplifies vocabulary augmentation by balancing efficiency and effectiveness, making it an ideal strategy for many domain-adaptive pretraining tasks.\n",
    "In the process of pretraining TaxBERT, we follow Webersinke et al. [2022] by augmenting the vocabulary of DistilRoBERTa through the addition of the 235 most common tokens in our tax corpus to the tokenizer, leading to a vocabulary increase from 50,265 to 50,500.\n",
    "\n",
    "Below you find a simple custom solution for identifying important tokens. First, we merge the entire corpus into a single string and then split it into individual tokens.\n",
  	 "Next, Python‚Äôs Counter counts how frequently each token appears in the corpus. However, each token is passed through the previously designed **clean_token function**.\n",
    "We further recommend filtering out tokens that are not suitable for adding to the tokenizer‚Äôs vocabulary. We keep only those tokens that appear at least 50 times in the corpus.\n",
    "Additionally, tokens that already exist in the tokenizer‚Äôs current vocabulary are excluded.\n",
    "As a further example showcased here, we also remove tokens that contain the special string 'num', tokens that are only one character long, tokens that contain any digits, and tokens that include parentheses or start or end with them.\n",
    "Note: Since the best filtering approach depends on the specific characteristics of the text corpus, we encourage experimenting with different strategies to optimize token selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11584e8b-9ec1-494e-a1c0-f958f940f48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_important_tokens(corpus, tokenizer, min_freq=50, max_vocab_size=50500):\n",
    "    all_tokens = ' '.join(corpus).split()\n",
    "    token_freq = Counter(clean_token(token) for token in all_tokens)\n",
    "    \n",
    "    # Filter tokens by min frequency, length, not being a number, excluding \"<num>\", and excluding parentheses\n",
    "    new_tokens = [\n",
    "        token for token, freq in token_freq.items() \n",
    "        if freq >= min_freq and \n",
    "           token not in tokenizer.get_vocab() and \n",
    "           '<num>' not in token and\n",
    "           len(token) > 1 and \n",
    "           not any(char.isdigit() for char in token) and\n",
    "           '(' not in token and ')' not in token and\n",
    "           not token.startswith('(') and not token.endswith(')')\n",
    "    ]\n",
    "    \n",
    "    # Sort the new tokens by frequency in descending order\n",
    "    sorted_new_tokens = sorted(new_tokens, key=lambda x: token_freq[x], reverse=True)\n",
    "    \n",
    "    # Calculate the number of new tokens to add\n",
    "    original_vocab_size = len(tokenizer.get_vocab())\n",
    "    num_new_tokens_to_add = max_vocab_size - original_vocab_size\n",
    "    \n",
    "    # Select the most frequent new tokens up to the allowed limit\n",
    "    important_tokens = sorted_new_tokens[:num_new_tokens_to_add]\n",
    "    \n",
    "    return important_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we initialize the relevant folders and create lists that will later store the final training and evaluation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321b0fe6-c858-4576-9852-c0379ed8279d",
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = [\n",
    "    r'C:\\enter_your_first_folder',\n",
    "    r'C:\\enter_your_second_folder'\n",
    " ]\n",
    "\n",
    "# Initialize lists to hold the final train and evaluation data\n",
    "train_texts = []\n",
    "eval_texts = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####**5. Redundancy Reduction**\n",
    "In the next step, we provide a simple yet effective solution for dealing with redundancy within a given corpus.\n",
    "Redundancy is a common characteristic in corporate reporting, as firms often operate closely within legal frameworks, leading to standardized language and repetitive phrasing across documents.\n",
    "To address this, we define **DataSelector** as class specifically designed to filter and select text samples based on diversity metrics.\n",
    "Upon initialization, it accepts two parameters: **keep**, which determines the proportion of texts to retain; **diversity_metrics**, a list specifying which diversity measures to apply.\n",
    "In addition, for purposes of demonstration, we also include the **tokenizer** parameter (although it is not currently used in the snippet).\n",
    "The DataSelector class employs two primary metrics to quantify text diversity: the Type-Token Ratio (TTR) and entropy.\n",
    "The TTR evaluates lexical variety by calculating the ratio of unique tokens to the total number of tokens within a text.\n",
    "In contrast, entropy measures the unpredictability or informational richness of a text, derived from the probability distribution of token frequencies.\n",
    "The fit method computes a diversity score for each text in the dataset by aggregating the selected metrics.\n",
    "Subsequently, the transform method ranks the texts based on these scores and retains a specified fraction of texts with the lowest diversity scores, effectively prioritizing less diverse or more uniform texts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6a12ca-2142-4ace-8988-4e6cbf585a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "class DataSelector:\n",
    "    def __init__(self, keep, tokenizer, diversity_metrics):\n",
    "        self.keep = keep\n",
    "        self.tokenizer = tokenizer\n",
    "        self.diversity_metrics = diversity_metrics\n",
    "\n",
    "    def _type_token_ratio(self, text):\n",
    "        tokens = text.split()\n",
    "        return len(set(tokens)) / len(tokens) if len(tokens) > 0 else 0\n",
    "\n",
    "    def _entropy(self, text):\n",
    "        token_counts = Counter(text.split())\n",
    "        total = sum(token_counts.values())\n",
    "        return -sum((count/total) * math.log2(count/total) for count in token_counts.values() if count > 0)\n",
    "\n",
    "    def fit(self, texts):\n",
    "        self.scores = []\n",
    "        for text in texts:\n",
    "            score = 0\n",
    "            if \"type_token_ratio\" in self.diversity_metrics:\n",
    "                score += self._type_token_ratio(text)\n",
    "            if \"euclidean\" in self.diversity_metrics:\n",
    "                score += self._entropy(text)\n",
    "            self.scores.append(score)\n",
    "\n",
    "    def transform(self, texts):\n",
    "        # Sort texts by their diversity scores\n",
    "        sorted_indices = np.argsort(self.scores)\n",
    "        n_keep = int(self.keep * len(texts))\n",
    "        # Keep the least similar texts (lower scores are kept)\n",
    "        selected_indices = sorted_indices[:n_keep]\n",
    "        return [texts[i] for i in selected_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we process the text corpus from multiple sources. The workflow begins by iterating over a set of predefined folders, each containing text files.\n",
    "For each folder, the script loads the raw text files using the **load_text_files** function and subsequently splits the texts into smaller, manageable chunks of 512 tokens via the **chunk_text function**.\n",
    "A critical step in the workflow is the selective application of the DataSelector class.\n",
    "If it is not intended for the DataSelector to iterate over each folder, simply specify the desired folder using an if statement.\n",
    "In our case, the DataSelector is configured to retain only 50% of the least similar text chunks, using diversity metrics such as the Type-Token Ratio and entropy.\n",
    "Once the texts are filtered, the script converts them into a structured dataset using the Dataset.from_dict method from the Hugging Face datasets library.\n",
    "Each dataset is then split into training and evaluation subsets, with 20% of the data reserved for evaluation to ensure robust model validation.\n",
    "The split is randomized using a **fixed seed for reproducibility**. Finally, the resulting training and evaluation texts from each folder are appended to their respective lists, aggregating data across different sources for comprehensive model training and assessment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a658cfd-9936-456a-96a2-bdbb6010ce92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tax corpus from different sources and split into chunks\n",
    "for folder in folders:\n",
    "    raw_texts = load_text_files(folder)\n",
    "    tax_texts = []\n",
    "    for text in raw_texts:\n",
    "        tax_texts.extend(chunk_text(text, chunk_size=512))\n",
    "\n",
    "    # Apply DataSelector to folders that end with _example\n",
    "    if folder.endswith('_example'):\n",
    "        selector = DataSelector(\n",
    "            keep=0.5,\n",
    "            tokenizer=tokenizer,\n",
    "            diversity_metrics=[\n",
    "                \"type_token_ratio\",\n",
    "                \"entropy\",\n",
    "            ],\n",
    "        )\n",
    "        selector.fit(tax_texts)\n",
    "        tax_texts = selector.transform(tax_texts)\n",
    "\n",
    "    # Create Dataset for the current folder\n",
    "    dataset = Dataset.from_dict({'text': tax_texts})\n",
    "    \n",
    "    # Split dataset into train and evaluation sets\n",
    "    train_test_split = dataset.train_test_split(test_size=0.2, seed=42) #Note: seed fixed for reproducibility\n",
    "    train_dataset = train_test_split['train']\n",
    "    eval_dataset = train_test_split['test']\n",
    "    \n",
    "    # Append the split datasets to the final lists\n",
    "    train_texts.extend(train_dataset['text'])\n",
    "    eval_texts.extend(eval_dataset['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step, we consolidate the previously processed data into training and evaluations datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d08288-2f4c-4bb2-aebf-fe13e50966de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and evaluation sets\n",
    "final_train_dataset = Dataset.from_dict({'text': train_texts})\n",
    "final_eval_dataset = Dataset.from_dict({'text': eval_texts})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####**6. Tokenization**\n",
    "The next script retrieves the original **vocabulary of the tokenizer** using get_vocab(). This vocabulary consists of all tokens that the pretrained model recognizes by default.\n",
    "Next, we call the previously defined function get_important_tokens to identify frequently occurring or semantically significant tokens that are critical for accurate text representation.\n",
    "These newly identified tokens are then added to the tokenizer‚Äôs vocabulary using **add_tokens()**.\n",
    "After expanding the vocabulary, the script calculates the difference between the new and original vocabularies to determine which tokens were successfully added.\n",
    "It also performs an intersection check to identify any tokens from the important set that were already present in the original vocabulary, ensuring that only truly novel terms are considered additions.\n",
    "**Finally, we recommend manually reviewing** the added tokens to assess whether they meaningfully extend the vocabulary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763ff7af-a6ec-4efb-b833-bf42982da669",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tokenizer and get important tokens\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilroberta-base')\n",
    "original_vocab = set(tokenizer.get_vocab().keys())\n",
    "\n",
    "# Call get_important_tokens\n",
    "important_tokens = get_important_tokens(tax_texts, tokenizer)\n",
    "tokenizer.add_tokens(important_tokens)\n",
    "new_vocab = set(tokenizer.get_vocab().keys())\n",
    "added_tokens = new_vocab - original_vocab\n",
    "\n",
    "# Check if any of the added tokens are in the original vocab\n",
    "tokens_already_in_vocab = original_vocab.intersection(important_tokens)\n",
    "tokens_not_in_vocab = set(important_tokens) - original_vocab\n",
    "\n",
    "print(\"Added tokens:\", added_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define and apply a **tokenize_function** to prepare text datasets for input into a transformer-based model.\n",
    "The script begins by defining the tokenize_function, which takes a batch of text samples and applies the tokenizer to each example.\n",
    "The tokenizer processes the text with specific parameters to standardize the input format.\n",
    "The function ensures that all tokenized sequences are padded to a uniform length using the padding='max_length' parameter.\n",
    "This uniformity is essential for efficient batch processing, as it allows the model to handle multiple inputs simultaneously without encountering dimensional inconsistencies.\n",
    "Additionally, the truncation=True setting ensures that any text exceeding the maximum allowable length is appropriately shortened.\n",
    "The maximum length is set to 512 tokens, aligning with the typical input size constraints of transformer models like BERT or RoBERTa, which ensures that all data conforms to the model‚Äôs architectural limitations.\n",
    "Once the tokenization function is defined, we apply it to both the training and evaluation datasets using the map method from the Hugging Face datasets library.\n",
    "By setting batched=True, the function processes multiple text samples in parallel, which significantly accelerates the tokenization process, especially when dealing with large datasets.\n",
    "The resulting tokenized datasets, **tokenized_train_dataset** and **tokenized_eval_dataset**, are now in a format suitable for model training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946bd4fb-7807-4e16-afa7-401e449684ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize function\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding='max_length', truncation=True, max_length=512)",
    "\n",
    "# Tokenize both datasets\n",
    "tokenized_train_dataset = final_train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_eval_dataset = final_eval_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####**7. Model Setup**\n",
    "Now, let‚Äôs start with domain-adaptive pretraining. First, we initialize a **data collator**.\n",
    "The DataCollatorForLanguageModeling is imported from the Hugging Face transformers library and is designed to dynamically apply masking to tokens within input sequences during the training process.\n",
    "By applying masks dynamically, the data collator ensures that the model is continuously exposed to new masking patterns in each epoch.\n",
    "We configure the data collator with the previously initialized tokenizer, ensuring that the tokenization scheme remains consistent throughout the preprocessing pipeline.\n",
    "The parameter mlm=True specifies that the collator will prepare data for masked language modeling.\n",
    "Note: You can also configure the mlm_probability parameter. A value of, for example, 0.10 indicates that 10% of the tokens in each input sequence will be randomly masked during training.\n",
    "Second, we load a pretrained transformer model and customize it for a masked language modeling (MLM) task.\n",
    "We begin by importing **RobertaConfig** and **RobertaForMaskedLM** from the Hugging Face transformers library.\n",
    "For many researchers, especially those in academic or smaller institutional settings, computational resources are limited and expensive to acquire.\n",
    "In line with Webersinke et al. [2022], we therefore recommend using **DistilRoBERTa** as the starting point for training.\n",
    "It is a distilled version of RoBERTa, which itself is an optimized version of the BERT model, enhancing performance across various NLP tasks.\n",
    "Thus, the RobertaConfig object is initialized using the configuration of DistilRoBERTa.\n",
    "To better adapt the model to the specific dataset and potentially address overfitting issues, we recommend modifying key hyperparameters within this configuration.\n",
    "Both the **hidden_dropout_prob** and **attention_probs_dropout_prob** can be increased (the default is 0.1; in this case, for example, they are increased to 0.3).\n",
    "The hidden dropout affects the feedforward layers of the model, while the attention dropout applies to the attention mechanisms that allow the model to focus on different parts of the input text.\n",
    "By increasing these dropout rates, the model is regularized more aggressively, which helps prevent overfitting. Overfitting is a common issue when fine-tuning on smaller or highly specialized datasets, such as those used in accounting and taxation research.\n",
    "We also recommend increasing these rates when dealing with repetitive language and limited variation, which might cause the model to memorize patterns rather than generalize effectively.\n",
    "The example script then loads the pretrained DistilRoBERTa model with the modified configuration. Next, the model's token embeddings are **resized** using model.resize_token_embeddings(len(tokenizer)).\n",
    "This step is crucial because the tokenizer was previously expanded to include new, domain-specific tokens not present in the original vocabulary.\n",
    "Finally, the model is moved to the specified GPU using model.to(device). This ensures that subsequent training steps leverage the available **GPU** resources for optimal performance.\n",
    "This stage sets the foundation for adapting a general-purpose language model to domain-specific texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093fab28-8d01-473c-b04f-ccd922b29043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data collator\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa86f69b-c27f-4833-907e-ca9da098bb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and resize token embeddings\n",
    "from transformers import RobertaConfig, RobertaForMaskedLM\n",
    "\n",
    "# Lade die Modellkonfiguration und passe die Dropout-Rate an\n",
    "config = RobertaConfig.from_pretrained('distilroberta-base')\n",
    "config.hidden_dropout_prob = 0.3\n",
    "config.attention_probs_dropout_prob = 0.3\n",
    "\n",
    "model = RobertaForMaskedLM.from_pretrained('distilroberta-base', config=config)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model.to(device)\n",
    "print(f\"Model is on device: {next(model.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####**8. Optimizer & Scheduler**\n",
    "Let‚Äôs set up the next stages. We import the **AdamW optimizer** from the Hugging Face transformers library.\n",
    "AdamW is an advanced optimization algorithm designed to adjust the model‚Äôs weights during training, ensuring that the model learns effectively from the data.\n",
    "The optimizer is configured to manage the model's parameters during training. The learning rate (here: lr=5e-5) controls how much the model‚Äôs weights are adjusted with each step.\n",
    "A learning rate that‚Äôs too high could make the model unstable, causing it to jump over the optimal solution, while a rate that‚Äôs too low would slow down the learning process.\n",
    "The eps parameter, known as epsilon, is a small number added to prevent division by zero during calculations. It ensures numerical stability without significantly affecting the learning process.\n",
    "The betas (here: 0.9, 0.999) parameters are coefficients for calculating moving averages of the gradients and their squares.\n",
    "These values help smooth out the learning process by balancing how much the optimizer relies on past gradients versus new information.\n",
    "This makes the learning process more stable, even when the data is noisy or complex, which is often the case in accounting and tax-related texts.\n",
    "Finally, weight_decay=0.01 adds regularization by penalizing large weights, helping the model generalize better to new, unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c542f6ba-1426-4a7c-a0ff-c41d7005950d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61f1990-cd5e-46b5-b7c7-6ebfd824d6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, eps=1e-6, betas=(0.9, 0.999), weight_decay=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next section of the code introduces an early stopping mechanism for your BERT model training, which is a critical feature to ensure the model doesn‚Äôt overfit your dataset.\n",
    "Overfitting occurs when a model learns patterns that are too specific to the training data and fails to generalize well to new, unseen data.\n",
    "The code starts by importing the necessary classes from the Hugging Face transformers library. These imports allow you to customize and control the training process.\n",
    "The **TrainerCallback** class serves as a base for creating hooks that can intervene at various stages of training, such as during evaluation or after each epoch.\n",
    "**TrainerState** holds the current state of training, and **TrainerControl** manages how the training process proceeds based on certain conditions.\n",
    "When initializing the **EarlyStoppingCallback** class, we define how patient the model should be before stopping training if no improvement occurs.\n",
    "The **early_stopping_patience** parameter, set to 1 here, means that if the model doesn‚Äôt improve after one evaluation cycle, it will stop training.\n",
    "The best_metric keeps track of the lowest evaluation loss achieved, while **patience_counter** counts how many consecutive times the model has failed to improve.\n",
    "The core functionality lies in the **on_evaluate** method. Every time the model evaluates its performance on the validation set, this method checks the evaluation loss.\n",
    "If the current evaluation loss is better (i.e., lower) than the best one recorded, it updates best_metric and resets the patience_counter to zero.\n",
    "However, if there‚Äôs no improvement, it increases the patience_counter by one. Once the patience counter exceeds the set threshold (early_stopping_patience), the training stops automatically by setting control.should_training_stop = True.\n",
    "This ensures that the model remains efficient and avoids overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93941030-e3fd-445b-9fc8-57fabfac59ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainerCallback, TrainerState, TrainerControl\n",
    "\n",
    "class EarlyStoppingCallback(TrainerCallback):\n",
    "    def __init__(self, early_stopping_patience=1):\n",
    "        self.early_stopping_patience = early_stopping_patience\n",
    "        self.best_metric = None\n",
    "        self.best_model_checkpoint = None\n",
    "        self.patience_counter = 0\n",
    "\n",
    "    def on_evaluate(self, args, state: TrainerState, control: TrainerControl, **kwargs):\n",
    "        logs = kwargs.get(\"logs\", {})\n",
    "        current_metric = logs.get(\"eval_loss\")\n",
    "\n",
    "        if self.best_metric is None or current_metric < self.best_metric:\n",
    "            self.best_metric = current_metric\n",
    "            self.best_model_checkpoint = state.global_step\n",
    "            self.patience_counter = 0\n",
    "        else:\n",
    "            self.patience_counter += 1\n",
    "\n",
    "        if self.patience_counter >= self.early_stopping_patience:\n",
    "            control.should_training_stop = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next part of the code sets up the **data loading** process, which is essential for efficiently feeding your dataset into the BERT model during both training and evaluation.\n",
    "First, we import the DataLoader class from PyTorch. DataLoader handles batching, shuffling, and loading data in parallel, optimizing the training process.\n",
    "Next, we create two data loaders: one for training and one for evaluation. The train_dataloader takes the tokenized_train_dataset and organizes it into batches of 64 samples.\n",
    "The shuffle=True argument ensures that the training data is randomly mixed at the start of each epoch.\n",
    "This randomness prevents the model from learning the sequence of the data, which helps improve generalization and reduce overfitting.\n",
    "The eval_dataloader handles the validation dataset (tokenized_eval_dataset) with the same batch size of 64, but without shuffling.\n",
    "Keeping the evaluation data in a consistent order ensures that model performance is measured reliably across epochs.\n",
    "This consistency is crucial when tracking improvements or identifying potential issues during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb343b2e-8e21-4566-9bfb-f385ea6d6ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55345d2e-000a-476c-a53a-8491b52e9c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(tokenized_train_dataset, batch_size=64, shuffle=True)\n",
    "eval_dataloader = DataLoader(tokenized_eval_dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we set the core training parameters. The **num_train_epochs** = 12 means the model will pass through the entire training dataset 12 times.\n",
    "This gives the model multiple opportunities to learn from the data, refining its understanding with each epoch. However, too many epochs can lead to overfitting, while too few may result in underfitting.\n",
    "The **train_batch_size** = 64 defines how many samples the model processes before updating its weights.\n",
    "A batch size of 64 is large enough to provide stable gradient estimates while still being efficient for most hardware setups.\n",
    "The **gradient_accumulation_steps** = 8 allows for accumulating gradients over multiple mini-batches before performing a backward pass to update the model weights.\n",
    "This effectively simulates a larger batch size without requiring additional memory.\n",
    "For instance, with a batch size of 64 and accumulation steps of 8, the model behaves as if it‚Äôs processing a batch size of 512. This is particularly useful when memory constraints are a limiting factor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d12804-5fe9-4ba7-ac77-f45abb293e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "num_train_epochs = 12\n",
    "train_batch_size = 64\n",
    "gradient_accumulation_steps = 8 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further, the code calculates how many steps your BERT model will take during training, which is crucial for managing the learning rate schedule and optimizing the training process.\n",
    "Here, **steps_per_epoch** is determined by dividing the total number of samples in the tokenized_train_dataset by the train_batch_size.\n",
    "The math.ceil function rounds this number up to ensure that all training data is included, even if the total number of samples isn‚Äôt perfectly divisible by the batch size.\n",
    "This ensures that every piece of data, including those in the last incomplete batch, is used during training. Next, num_train_epochs represents how many times the model will go through the entire dataset.\n",
    "Multiplying this by steps_per_epoch gives the total number of steps for the entire training process.\n",
    "Since gradient_accumulation_steps is used to accumulate gradients over several mini-batches before updating the model weights, dividing by this number adjusts the total to reflect how many actual weight updates will occur.\n",
    "Finally, the **warmup_steps** parameter controls how many steps will be used to gradually increase the learning rate at the start of training.\n",
    "Setting it to 2.5% of the total training steps means that the learning rate will start small and slowly ramp up over the first 2.5% of the training process.\n",
    "This gradual warm-up helps prevent large, unstable updates at the beginning of training, which could otherwise cause the model to perform poorly or even fail to converge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d12804-5fe9-4ba7-ac77-f45abb293e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate total training steps\n",
    "steps_per_epoch = math.ceil(len(tokenized_train_dataset) / train_batch_size)\n",
    "total_training_steps = num_train_epochs * steps_per_epoch // gradient_accumulation_steps\n",
    "warmup_steps = int(0.025 * total_training_steps)  # 2,5% of total training steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next part of our code sets up a custom **learning rate scheduler**, which controls how the learning rate changes throughout the training process.\n",
    "While get_linear_schedule_with_warmup is a standard scheduler from the Hugging Face Transformers library, our script demonstrates the possibility of implementing a custom schedule using LambdaLR from PyTorch.\n",
    "**LambdaLR** allows for defining a flexible learning rate function through the lr_lambda parameter. **initial_lr** represents the learning rate after the warm-up phase, and **final_lr** is the rate the model will decay towards as training progresses.\n",
    "Starting with a lower learning rate (5e-6) helps stabilize the model in the early stages, especially when fine-tuning on sensitive financial data.\n",
    "The learning rate then increases and eventually decays towards 5e-5 as training advances. The lr_lambda function defines how the learning rate changes at each training step.\n",
    "In the warm-up phase, when current_step is less than warmup_steps, the learning rate increases linearly from a defined starting point (here: 0 as an example) to initial_lr.\n",
    "After the warm-up period, the learning rate begins to decay linearly from initial_lr towards final_lr.\n",
    "The decay is based on the progress of training, calculated as the ratio of completed steps to total training steps.\n",
    "This gradual reduction in the learning rate allows the model to make finer adjustments as it converges, helping it settle into an optimal state without overshooting.\n",
    "Finally, we connect the custom lr_lambda function to the optimizer, ensuring that the learning rate updates at every step according to the defined schedule.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f51c8d2-6c6e-4551-83d9-26f2ff48769d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "# Learning rate schedule parameters\n",
    "initial_lr = 5e-6\n",
    "final_lr = 5e-5\n",
    "\n",
    "def lr_lambda(current_step: int):\n",
    "    if current_step < warmup_steps:\n",
    "        # Linear warmup from 0 to initial_lr\n",
    "        return float(current_step) / float(max(1, warmup_steps))\n",
    "    else:\n",
    "        # Linear decay from initial_lr to final_lr\n",
    "        progress = float(current_step - warmup_steps) / float(max(1, total_training_steps - warmup_steps))\n",
    "        return max(0.0, initial_lr - progress * (initial_lr - final_lr)) / initial_lr\n",
    "\n",
    "scheduler = LambdaLR(optimizer, lr_lambda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As **alternative**, you could also set up a **cosine learning rate scheduler** with warm-up.\n",
    "This scheduler adjusts the learning rate following a cosine function, which means the learning rate starts high, gradually decreases, and then flattens out as training progresses.\n",
    "This pattern helps the model converge more smoothly compared to linear decay schedules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e944a9eb-9701-404a-979a-c0cee171b849",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_cosine_schedule_with_warmup\n",
    "scheduler = get_cosine_schedule_with_warmup(optimizer, warmup_steps, total_training_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####**9. Reproducible Training**\n",
    "The following code defines the **training parameters for pretraining** a transformer-based model on a specific domain using the TrainingArguments class from the Hugging Face transformers library.\n",
    "Hyperparameters determine the configuration and behavior of the model during training, influencing how effectively it learns from the domain-specific corpus.\n",
    "\n",
    "First, the **number of epochs** refers to the total number of complete passes the model makes through the training dataset during pretraining.\n",
    "A sufficient number of epochs allows the model to learn complex patterns and relationships within the domain-specific corpus.\n",
    "However, too many epochs can lead to overfitting, where the model memorizes the training data instead of generalizing well to unseen data.\n",
    "For domain-adaptive pretraining, the number of epochs is typically set between three and 20, depending on the dataset‚Äôs size and complexity.\n",
    "\n",
    "Second, the effective **batch size** specifies the total number of training samples used collectively to update the model‚Äôs parameters.\n",
    "There is a trade-off when selecting a batch size: larger batch sizes improve computational efficiency and provide smoother gradient updates but require more memory, whereas smaller batch sizes introduce more noise into updates, which can sometimes enhance generalization but slow down convergence.\n",
    "The effective batch size is determined by the batch size per device and gradient accumulation, a technique that accumulates gradients over multiple steps before updating the model.\n",
    "This approach simulates a larger batch size, stabilizing training.\n",
    "\n",
    "To speed up data loading, dataloader_num_workers is set to 32, leveraging multiprocessing to fetch training samples in parallel. This is particularly beneficial when working with large datasets.\n",
    "Mixed-precision training (fp16=True) is enabled, allowing the model to use lower-precision floating-point arithmetic. This significantly accelerates computations on GPUs while reducing memory consumption.\n",
    "\n",
    "Third, the **learning rate** controls the step size at which the model updates its parameters (weights) during training in response to computed gradients.\n",
    "It determines how quickly the model learns from the data. Choosing the optimal learning rate is challenging:\n",
    "if it is too low, the model takes very small steps, resulting in slow training.\n",
    "If it is too high, the model may take excessively large steps, potentially overshooting the optimal point in the loss function, which measures the difference between the model‚Äôs predictions and the true labels.\n",
    "\n",
    "Finally, setting the **random seed** to 42 ensures **reproducibility**, making it possible to obtain consistent results when re-running the training process.\n",
    "\n",
    "We further recommend performing a **grid-search** on the ranges œµ {2e-4, 2e-5, 3e-4, 3e-5, 4e-4, 4e-5, 5e-4, 5e-5} for learning rate and œµ {8, 16, 32, 64} for batch size.\n",
    "This hyperparameter tuning method systematically explores different values for these parameters, identifying the combination that yields the best performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25bb288c-f3f8-494d-931a-e828aaa776a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=r'C:\\insert_your_path\\Results',\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=12,\n",
    "    per_device_train_batch_size=32,  # Bigger batch sizes for bigger GPUs\n",
    "    gradient_accumulation_steps=16,  # Accumulate gradients to simulate larger batch size\n",
    "    dataloader_num_workers=32,  # Number of subprocesses to use for data loading\n",
    "    save_steps=500,\n",
    "    save_total_limit=3,\n",
    "    fp16=True,  # Enable mixed precision for more efficient training\n",
    "    logging_dir=r'C:\\insert_your_path\\Logs',  # Specify a directory for logs\n",
    "    eval_strategy=\"epoch\",  # Evaluate the model at epochs\n",
    "    warmup_steps=warmup_steps,  # Number of warmup steps as previously defined\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.001,\n",
    "    seed=42 #crucial for reproducibility\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####**10. Model Evaluation**\n",
    "The remainder is straightforward: initialize the trainer, train, and save the trained model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a0e72e-164e-4871-b5a1-1ad70186a0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_eval_dataset,\n",
    "    optimizers=(optimizer, scheduler),\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3ed1a3-fc82-470e-8eb3-28f95bed1dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c6868d-528d-4302-84d5-03d6638f21c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3219be9e-e70a-48ba-94b1-56c0dc4778db",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(r'C:\\your_path\\Model')\n",
    "tokenizer.save_pretrained(r'C:\\your_path\\Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec86d21-c45c-46d8-bba5-f132aa23b93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8901ebd-6ea9-4dfa-8cd4-2d6a1fe98eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(r\"C:\\your_path\\Model\\model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MIT License\n",
    "Copyright (c) 2025 Marius Wei√ü\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "of this software and associated documentation files (the Software), to deal\n",
    "in the Software without restriction, including without limitation the rights\n",
    "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "copies of the Software, and to permit persons to whom the Software is\n",
    "furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all\n",
    "copies or substantial portions of the Software.\n",
    "##### If the Software is used for academic or scientific purposes, cite the paper Hechtner et al., (2025) How to Design and Employ Specialized Large Language Models for Accounting and Tax Research: The Example of TaxBERT.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED **AS IS**, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM\n",
    "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "SOFTWARE.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
