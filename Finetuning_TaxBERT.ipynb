{
 "cells": [
 {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####        This GitHub Repository accompanies the Paper\n",
    "## **How to Design and Employ Specialized Large Language Models for Accounting and Tax Research: The Example of TaxBERT**\n",
    "**Frank Hechtner, Lukas Schmidt, Andreas Seebeck, and Marius Weiß**\n",
    "##### If the following Guide/Repository is used for academic or scientific purposes, please cite the paper Hechtner et al., (2025) How to Design and Employ Specialized Large Language Models for Accounting and Tax Research: The Example of TaxBERT.\n",
    "##### Link to paper: [SSRN](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5146523)\n",
    "##### Version as of February 2025\n",
    "## Part 2 of example code: Fine-tuning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook aims to give a comprehensive guide for fine-tuning the previously pretrained LLM.\n",
    "\n",
    "1. **Library Imports**: Import necessary libraries from PyTorch and Hugging Face for handling large language models (LLMs) and datasets.\n",
    "\n",
    "2. **Reproducibilty**: Set the random seed to a fixed number.\n",
    "\n",
    "3. **Set-up examples in Excel**: Organize the examples in Excel and store them in lists.\n",
    "\n",
    "4. **Create dataset**: Define a custom dataset class.\n",
    "\n",
    "5. **Create training and validation dataset**: Split the dataset in training and validation.\n",
    "\n",
    "6. **Fine-tuning**: Run the fine-tuning and evaluation process.\n",
    "\n",
    "7. **Save the model**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####**1. Library Imports**\n",
    "The following libraries and imports are required to start. Additionally, **PyTorch, Huggingface transformers and NVIDIA CUDA** are mandatory dependencies.\n",
    "**Note**: The example code requires Python 3.9 or later. It is tested on the stable PyTorch 2.6.0, Transformers 4.48.3, and NVIDIA CUDA 12.4.\n",
    "We cannot guarantee the stability for newer or older versions of these packages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838890d9-9fa5-4405-8e97-3fd4da2cb7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.metrics import f1_score  # ggf. später: classification_report, roc_auc_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####**2. Reproducibilty**\n",
    "Setting the **random seed** to a fixed number (here: 42) ensures **reproducibility**, making it possible to obtain consistent results when re-running the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d81a372-f12d-447e-8325-1a8cdaebf9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "os.environ["TOKENIZERS_PARALLELISM"] = 'false'\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####**3. Set up examples in Excel**\n",
    "We recommend organizing the examples in Excel with only two columns.\n",
    "The Text column, which contains sentences/paragraphs/documents, is converted into a list called texts.\n",
    "The Label column contains categorical classifications (e.g., tax-related vs. not tax-related)\n",
    "Both columns are stored in separate lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c7140b-1ee7-43cc-83bb-9898cae9dcb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in your examples\n",
    "data = pd.read_excel(r'C:\\specify_your_path.xlsx')\n",
    "\n",
    "# two lists for texts and labels\n",
    "texts = data['Text'].tolist()\n",
    "labels = data['Label'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####**4. Create dataset**\n",
    "The AutoTokenizer processes the text data, applying truncation to ensure that inputs do not exceed the maximum sequence length of 512 tokens,\n",
    "padding shorter sequences to maintain uniform input size, and converting the text into tensors formatted for PyTorch **return_tensors=pt**.\n",
    "This transformation is essential because transformer models operate on numerical tensor representations, and not on raw text.\n",
    "Next, a custom **dataset class**, TextDataset, is defined, inheriting from PyTorchs Dataset class.\n",
    "This class takes the tokenized encodings and corresponding labels as input and structures them into a format that can be efficiently fed into the model.\n",
    "Within the dataset class, the **__getitem__** method ensures that each sample is accessed as a dictionary containing both the tokenized text and its corresponding label, which is explicitly converted into a PyTorch tensor of type long.\n",
    "The **__len__** method provides the total number of samples in the dataset.\n",
    "Finally, an instance of TextDataset is created using the processed encodings and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdba6a57-cf28-4bf7-95a2-bf4e36c002be",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "encodings = tokenizer(texts, truncation=True, padding=True, max_length=512, return_tensors=\"pt\")\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)  # Typ festlegen\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "dataset = TextDataset(encodings, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####**5. Create training and validation dataset**\n",
    "The next step is responsible for splitting the dataset into training and validation subsets and preparing it for efficient batch processing during model training and evaluation.\n",
    "It also initializes key variables to track model performance over multiple evaluation rounds.\n",
    "The previously defined dataset is first divided into two subsets.\n",
    "The training set comprises 80% of the data, while the remaining 20% is allocated to validation.\n",
    "This is done using the random_split function from PyTorch, ensuring that the split is randomized to prevent any bias in training or validation. Note: For exact reproducibility,\n",
    "you will need to set a fixed seed.\n",
    "Once the datasets are created, they are wrapped in DataLoader (**train_loader** and **val_loader**) objects, which facilitate efficient batch processing.\n",
    "Let's initializes a few variables.\n",
    "num_evaluations is set to 1.\n",
    "Two empty lists, val_losses and val_f1_scores, are also initialized.\n",
    "These will store the validation loss and F1 scores, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b7581c-6f20-47b2-8b6e-33fc27e29852",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64)\n",
    "\n",
    "num_evaluations = 1\n",
    "val_losses = []\n",
    "val_f1_scores = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####**6. Fine-tuning**\n",
    "Let's run the fine-tuning and evaluation process. The script begins by determining whether a GPU (CUDA) is available and sets the computing device accordingly.\n",
    "The model, is loaded from your specified path (see Code part 1 step 10!) and moved to **GPU**.\n",
    "Again, **AdamW** is the optimizer.\n",
    "The training process consists of multiple epochs, each involving two main steps: training and validation.\n",
    "During **training**, the model iterates over the batches of data, computing the loss, performing backpropagation, and updating the model’s parameters.\n",
    "The loss is accumulated to monitor training progress.\n",
    "After each epoch, the model is **evaluated** on the validation set. The **validation loss**, **F1 score**, and **accuracy** are calculated.\n",
    "To prevent overfitting, we employ **early stopping** by tracking the validation loss.\n",
    "If no improvement is observed over multiple epochs, training halts early to avoid unnecessary computations.\n",
    "If a new best validation loss is achieved, the model's parameters are saved.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca43ee72-9320-4260-bd23-57629cc8f4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f52df7-9c8e-4b02-bff5-207ba76d774a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = r'C:\\specify_your_models_path'\n",
    "\n",
    "for evaluation in range(num_evaluations):\n",
    "    print(f'Starting Evaluation Run {evaluation + 1}/{num_evaluations}')\n",
    "\n",
    "    # initialize model\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "    model.config.problem_type = \"single_label_classification\"\n",
    "    model.to(device)\n",
    "    optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "    \n",
    "    # Early stopping\n",
    "    patience = 4\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    num_epochs = 5\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        for batch in tqdm(train_loader, desc=f'Training Epoch {epoch + 1}'):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_train_loss += loss.item()\n",
    "        \n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        \n",
    "        # evaluation after each epoch\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_f1 = 0\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                batch = {k: v.to(device) for k, v in batch.items()}\n",
    "                outputs = model(**batch)\n",
    "                val_loss += outputs.loss.item()\n",
    "\n",
    "                predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "                f1 = f1_score(batch['labels'].cpu(), predictions.cpu(), average='weighted')\n",
    "                val_f1 += f1\n",
    "\n",
    "                total += batch['labels'].size(0)\n",
    "                correct += (predictions == batch['labels']).sum().item()\n",
    "        \n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        avg_val_f1 = val_f1 / len(val_loader)\n",
    "        val_accuracy = correct / total\n",
    "\n",
    "        print(f'Epoch {epoch + 1}: Training Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}, Validation F1 Score: {avg_val_f1:.4f}, Validation Accuracy: {val_accuracy:.4f}')\n",
    "\n",
    "        # early stopping\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            epochs_no_improve = 0\n",
    "            # optional: save the best model\n",
    "            torch.save(model.state_dict(), f'best_model_run{evaluation+1}.pt')\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= patience:\n",
    "                print('Early stopping triggered')\n",
    "                break\n",
    "\n",
    "    val_losses.append(best_val_loss)\n",
    "    val_f1_scores.append(avg_val_f1)\n",
    "\n",
    "mean_val_loss = np.mean(val_losses)\n",
    "std_val_loss = np.std(val_losses)\n",
    "mean_val_f1 = np.mean(val_f1_scores)\n",
    "std_val_f1 = np.std(val_f1_scores)\n",
    "\n",
    "print(f'Validation Loss (Mean ± Std): {mean_val_loss:.4f} ± {std_val_loss:.4f}')\n",
    "print(f'Validation F1 Score (Mean ± Std): {mean_val_f1:.4f} ± {std_val_f1:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####**7. Save the model**\n",
    "In the last step, we save the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5679326b-9bbf-4e01-b18c-9247df7cc586",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = r'C:\\specify_your_path_here'\n",
    "model.save_pretrained(save_path)\n",
    "tokenizer.save_pretrained(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MIT License\n",
    "Copyright (c) 2025 Marius Weiß\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "of this software and associated documentation files (the Software), to deal\n",
    "in the Software without restriction, including without limitation the rights\n",
    "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "copies of the Software, and to permit persons to whom the Software is\n",
    "furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all\n",
    "copies or substantial portions of the Software.\n",
    "##### If the Software is used for academic or scientific purposes, cite the paper Hechtner et al., (2025) How to Design and Employ Specialized Large Language Models for Accounting and Tax Research: The Example of TaxBERT.\n",
    "\n",
    "\n",
    "THE SOFTWARE IS PROVIDED **AS IS**, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM\n",
    "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "SOFTWARE.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
